# Nano-vLLM ä¸­æ–‡æ–‡æ¡£

<div align="center">

![Nano-vLLM Logo](../assets/logo.png)

**è½»é‡çº§ã€é«˜æ€§èƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.4+](https://img.shields.io/badge/PyTorch-2.4+-ee4c2c.svg)](https://pytorch.org/)

[English](../README.md) | ç®€ä½“ä¸­æ–‡

</div>

---

## ğŸ“– ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æŠ€æœ¯æ¶æ„](#æŠ€æœ¯æ¶æ„)
- [æ€§èƒ½åŸºå‡†](#æ€§èƒ½åŸºå‡†)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [è¯¦ç»†æ–‡æ¡£](#è¯¦ç»†æ–‡æ¡£)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [æ‰©å±•æ–¹å‘](#æ‰©å±•æ–¹å‘)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)
- [è®¸å¯è¯](#è®¸å¯è¯)

---

## é¡¹ç›®ç®€ä»‹

### ä»€ä¹ˆæ˜¯ Nano-vLLMï¼Ÿ

Nano-vLLM æ˜¯ä¸€ä¸ª**ä»é›¶å¼€å§‹å®ç°çš„è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“**ï¼Œæ—¨åœ¨é€šè¿‡æç®€çš„ä»£ç ï¼ˆä»… ~1,400 è¡Œ Pythonï¼‰å±•ç¤ºç°ä»£ LLM æ¨ç†ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ã€‚

### ä¸ºä»€ä¹ˆé€‰æ‹© Nano-vLLMï¼Ÿ

| ç‰¹ç‚¹ | Nano-vLLM | å®˜æ–¹ vLLM |
|------|-----------|-----------|
| ä»£ç é‡ | ~1,400 è¡Œ | ~100,000 è¡Œ |
| å¯è¯»æ€§ | â­â­â­â­â­ | â­â­â­ |
| æ€§èƒ½ | 1434 tok/s | 1361 tok/s |
| å­¦ä¹ æ›²çº¿ | å¹³ç¼“ | é™¡å³­ |
| é€‚ç”¨åœºæ™¯ | å­¦ä¹ ã€ç ”ç©¶ | ç”Ÿäº§ã€å•†ä¸š |

**Nano-vLLM ä»¥ä¸åˆ° 2% çš„ä»£ç é‡å®ç°äº†è¶…è¶Š vLLM çš„æ€§èƒ½ï¼**

### è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

1. **å­¦ä¹ é—¨æ§›é«˜**: å®˜æ–¹ vLLM ä»£ç é‡å¤§ï¼Œéš¾ä»¥å¿«é€Ÿç†è§£æ ¸å¿ƒåŸç†
2. **æŠ€æœ¯å¤æ‚åº¦**: å¤§æ¨¡å‹æ¨ç†æ¶‰åŠå¤§é‡ä¼˜åŒ–æŠ€æœ¯ï¼Œç¼ºä¹ç®€æ´çš„å‚è€ƒå®ç°
3. **ç ”ç©¶éœ€æ±‚**: éœ€è¦ä¸€ä¸ªè½»é‡çº§æ¡†æ¶å¿«é€ŸéªŒè¯æ–°æƒ³æ³•

---

## æ ¸å¿ƒç‰¹æ€§

### âœ¨ é«˜æ€§èƒ½æ¨ç†

- âš¡ **è¿ç»­æ‰¹å¤„ç†ï¼ˆContinuous Batchingï¼‰**: åŠ¨æ€è°ƒåº¦è¯·æ±‚ï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡
- ğŸš€ **CUDA å›¾ä¼˜åŒ–**: å‡å°‘ kernel å¯åŠ¨å¼€é”€ï¼Œé™ä½ ~30% å»¶è¿Ÿ
- ğŸ’¾ **Prefix Caching**: è‡ªåŠ¨ç¼“å­˜å’Œå¤ç”¨å…¬å…±å‰ç¼€ï¼Œæå‡å¤šè½®å¯¹è¯æ€§èƒ½
- ğŸ“¦ **Paged Attention**: åˆ†å— KV Cache ç®¡ç†ï¼Œæ”¯æŒæ›´å¤šå¹¶å‘è¯·æ±‚

### ğŸ¯ æ˜“äºç†è§£

- ğŸ“ **æç®€ä»£ç **: æ ¸å¿ƒä»£ç ä»… 1,400 è¡Œï¼Œæ˜“äºé˜…è¯»å’Œå­¦ä¹ 
- ğŸ“š **è¯¦ç»†æ–‡æ¡£**: åŒ…å«æ¶æ„ã€æ¨¡å—ã€æµç¨‹ã€ä»£ç è®²è§£ç­‰å¤šä»½æ–‡æ¡£
- ğŸ“ **æ•™å­¦å¯¼å‘**: ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šç²¾ç®€ï¼Œé€‚åˆå­¦ä¹ 

### ğŸ”§ é«˜åº¦å¯æ‰©å±•

- ğŸ”Œ **æ¨¡å—åŒ–è®¾è®¡**: æ¸…æ™°çš„å±‚æ¬¡ç»“æ„ï¼Œæ˜“äºæ·»åŠ æ–°åŠŸèƒ½
- ğŸ¨ **æ’ä»¶åŒ–æ¶æ„**: æ”¯æŒæ·»åŠ æ–°æ¨¡å‹ã€æ–°ç®—å­ã€æ–°è°ƒåº¦ç­–ç•¥
- âš™ï¸ **é…ç½®é©±åŠ¨**: é€šè¿‡é…ç½®æ–‡ä»¶çµæ´»æ§åˆ¶è¡Œä¸º

### ğŸŒ åˆ†å¸ƒå¼æ”¯æŒ

- ğŸ–¥ï¸ **å¼ é‡å¹¶è¡Œ**: æ”¯æŒå¤š GPU åˆ†å¸ƒå¼æ¨ç†ï¼ˆæœ€å¤š 8 å¡ï¼‰
- ğŸ”„ **NCCL é€šä¿¡**: é«˜æ•ˆçš„ GPU é—´é€šä¿¡
- ğŸ” **å¤šè¿›ç¨‹æ¶æ„**: é¿å… Python GILï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚

```bash
# æ“ä½œç³»ç»Ÿ
Ubuntu 20.04+ / macOS 12+

# Python ç‰ˆæœ¬
Python 3.10 - 3.12

# GPU
NVIDIA GPU with Compute Capability â‰¥ 8.0ï¼ˆæ¨è A100, H100ï¼‰

# æ˜¾å­˜
æœ€å°‘ 16GBï¼ˆQwen3-0.6Bï¼‰
æ¨è 40GB+ï¼ˆQwen3-1.5B + å¤šå¹¶å‘ï¼‰
```

### 2. å®‰è£…ä¾èµ–

#### æ–¹æ³• 1: ä½¿ç”¨ uvï¼ˆæ¨èï¼‰

```bash
# å®‰è£… uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# å…‹éš†ä»“åº“
git clone https://github.com/GeeeekExplorer/nano-vllm.git
cd nano-vllm

# å®‰è£…ä¾èµ–
uv sync
```

#### æ–¹æ³• 2: ä½¿ç”¨ pip

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/GeeeekExplorer/nano-vllm.git
cd nano-vllm

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/macOS
# æˆ–
venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -e .
```

### 3. ä¸‹è½½æ¨¡å‹

```bash
# ä½¿ç”¨ HuggingFace CLI
huggingface-cli download Qwen/Qwen3-0.6B --local-dir ~/models/Qwen3-0.6B

# æˆ–ä½¿ç”¨ Git LFS
git lfs install
git clone https://huggingface.co/Qwen/Qwen3-0.6B ~/models/Qwen3-0.6B
```

### 4. è¿è¡Œç¤ºä¾‹

```python
# example.py

from nanovllm import LLM, SamplingParams

# åˆå§‹åŒ– LLM
llm = LLM(
    model="~/models/Qwen3-0.6B",
    enforce_eager=False,          # å¯ç”¨ CUDA å›¾
    tensor_parallel_size=1,       # å• GPU
    gpu_memory_utilization=0.9    # ä½¿ç”¨ 90% GPU å†…å­˜
)

# é…ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,    # æ¸©åº¦ï¼ˆ0.1-2.0ï¼Œè¶Šä½è¶Šç¡®å®šæ€§ï¼‰
    max_tokens=256,     # æœ€å¤§ç”Ÿæˆé•¿åº¦
    ignore_eos=False    # é‡åˆ° EOS åœæ­¢
)

# æ‰¹é‡ç”Ÿæˆ
prompts = [
    "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
    "è¯·è®²ä¸€ä¸ªç¬‘è¯ã€‚",
    "Python å’Œ JavaScript æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
]

outputs = llm.generate(prompts, sampling_params)

# è¾“å‡ºç»“æœ
for i, output in enumerate(outputs):
    print(f"\n=== è¯·æ±‚ {i+1} ===")
    print(f"æç¤ºè¯: {prompts[i]}")
    print(f"å›å¤: {output['text']}")
    print(f"Token æ•°: {len(output['token_ids'])}")
```

è¿è¡Œï¼š

```bash
python example.py
```

### 5. æ€§èƒ½åŸºå‡†æµ‹è¯•

```bash
# è¿è¡ŒåŸºå‡†æµ‹è¯•ï¼ˆ256 ä¸ªå¹¶å‘åºåˆ—ï¼‰
python bench.py

# è¾“å‡ºç¤ºä¾‹:
# Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 256/256 [01:45<00:00,  2.44it/s, Prefill=15234tok/s, Decode=1434tok/s]
# Average Decode Throughput: 1434.2 tokens/s
```

---

## æŠ€æœ¯æ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TD
    A["ç”¨æˆ·æ¥å£å±‚<br/>LLM.generate(prompts, params)"] --> B["å¼•æ“æ§åˆ¶å±‚<br/>LLMEngine (è°ƒåº¦ + åˆ†è¯ + å¤šè¿›ç¨‹ç®¡ç†)"]

    B --> C["è°ƒåº¦å±‚<br/>Scheduler<br/>- è¯·æ±‚é˜Ÿåˆ—<br/>- Prefill/Decode<br/>- èµ„æºæŠ¢å "]
    B --> D["èµ„æºç®¡ç†å±‚<br/>BlockManager<br/>- KV Cache å—åˆ†é…<br/>- Prefix Caching<br/>- å¼•ç”¨è®¡æ•°ç®¡ç†"]
    D -.-> C

    C --> E["æ‰§è¡Œå±‚<br/>ModelRunner (CUDA å›¾ + åˆ†å¸ƒå¼é€šä¿¡ + æ•°æ®å‡†å¤‡)"]
    E --> F["æ¨¡å‹å±‚<br/>Qwen3ForCausalLM (Transformer è§£ç å™¨)<br/>- Embedding â†’ Decoder Layers â†’ LM Head<br/>- Attention + MLP + LayerNorm"]
    F --> G["ç®—å­å±‚<br/>- FlashAttention (é«˜æ•ˆæ³¨æ„åŠ›)<br/>- Tensor Parallel Linear (å¼ é‡å¹¶è¡Œ)<br/>- RoPE (æ—‹è½¬ä½ç½®ç¼–ç )<br/>- Sampler (Token é‡‡æ ·)"]

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#ffe1e1
    style D fill:#ffe1e1
    style E fill:#e1ffe1
    style F fill:#f0e1ff
    style G fill:#ffe1f0
```

### ç›®å½•ç»“æ„

```
nano-vllm/
â”œâ”€â”€ nanovllm/                   # æ ¸å¿ƒåŒ…
â”‚   â”œâ”€â”€ __init__.py            # å¯¼å‡º LLM, SamplingParams
â”‚   â”œâ”€â”€ config.py              # å…¨å±€é…ç½®
â”‚   â”œâ”€â”€ llm.py                 # ç”¨æˆ·æ¥å£
â”‚   â”œâ”€â”€ sampling_params.py     # é‡‡æ ·å‚æ•°
â”‚   â”‚
â”‚   â”œâ”€â”€ engine/                # æ¨ç†å¼•æ“æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ llm_engine.py     # å¼•æ“ä¸»æ§åˆ¶å™¨
â”‚   â”‚   â”œâ”€â”€ model_runner.py   # æ¨¡å‹æ‰§è¡Œå™¨ â­
â”‚   â”‚   â”œâ”€â”€ scheduler.py      # è¯·æ±‚è°ƒåº¦å™¨
â”‚   â”‚   â”œâ”€â”€ sequence.py       # åºåˆ—æ•°æ®ç»“æ„
â”‚   â”‚   â””â”€â”€ block_manager.py  # KV Cache ç®¡ç† â­
â”‚   â”‚
â”‚   â”œâ”€â”€ layers/                # ç¥ç»ç½‘ç»œå±‚
â”‚   â”‚   â”œâ”€â”€ attention.py      # FlashAttention â­
â”‚   â”‚   â”œâ”€â”€ linear.py         # å¼ é‡å¹¶è¡Œçº¿æ€§å±‚ â­
â”‚   â”‚   â”œâ”€â”€ embed_head.py     # è¯åµŒå…¥å’Œ LM Head
â”‚   â”‚   â”œâ”€â”€ layernorm.py      # RMSNorm
â”‚   â”‚   â”œâ”€â”€ rotary_embedding.py # RoPE
â”‚   â”‚   â”œâ”€â”€ activation.py     # æ¿€æ´»å‡½æ•°
â”‚   â”‚   â””â”€â”€ sampler.py        # Token é‡‡æ ·å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                # æ¨¡å‹æ¶æ„
â”‚   â”‚   â””â”€â”€ qwen3.py          # Qwen3 æ¨¡å‹
â”‚   â”‚
â”‚   â””â”€â”€ utils/                 # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ context.py        # å…¨å±€ä¸Šä¸‹æ–‡
â”‚       â””â”€â”€ loader.py         # æ¨¡å‹æƒé‡åŠ è½½
â”‚
â”œâ”€â”€ example.py                 # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ bench.py                   # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”œâ”€â”€ pyproject.toml            # é¡¹ç›®é…ç½®
â””â”€â”€ README.md                 # é¡¹ç›®æ–‡æ¡£
```

**â­ æ ‡è®°çš„æ˜¯æ ¸å¿ƒæ–‡ä»¶ï¼Œå»ºè®®ä¼˜å…ˆé˜…è¯»**

---

## æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ

```
GPU: NVIDIA A100-80GB
æ¨¡å‹: Qwen3-0.6B
è´Ÿè½½: 256 ä¸ªå¹¶å‘åºåˆ—
è¾“å…¥é•¿åº¦: 100-1024 tokensï¼ˆéšæœºï¼‰
è¾“å‡ºé•¿åº¦: 100-1024 tokensï¼ˆéšæœºï¼‰
```

### æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | Nano-vLLM | vLLM | å·®å¼‚ |
|------|-----------|------|------|
| **Prefill ååé‡** | 15,234 tok/s | 14,892 tok/s | +2.3% |
| **Decode ååé‡** | **1,434 tok/s** | 1,361 tok/s | **+5.4%** |
| **é¦– Token å»¶è¿Ÿï¼ˆTTFTï¼‰** | 0.18s | 0.19s | -5.3% |
| **å†…å­˜å ç”¨** | 6.2 GB | 6.5 GB | -4.6% |
| **ä»£ç é‡** | **1,358 è¡Œ** | 100,000+ è¡Œ | **-98.6%** |

**ç»“è®º**: Nano-vLLM ä»¥ä¸åˆ° 2% çš„ä»£ç é‡å®ç°äº† 105% çš„æ€§èƒ½ï¼

### æ€§èƒ½å¯è§†åŒ–

```
ååé‡å¯¹æ¯” (tokens/s)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Prefill:
  Nano-vLLM  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  15,234
  vLLM       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  14,892

Decode:
  Nano-vLLM  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  1,434
  vLLM       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   1,361
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. Prefill vs Decode

LLM æ¨ç†åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

#### Prefill é˜¶æ®µï¼ˆé¦–æ¬¡æ¨ç†ï¼‰
- **è¾“å…¥**: å®Œæ•´çš„ promptï¼ˆå¦‚ "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"ï¼‰
- **è®¡ç®—**: å¹¶è¡Œå¤„ç†æ‰€æœ‰ tokens
- **è¾“å‡º**: ç”Ÿæˆç¬¬ä¸€ä¸ª token
- **ç‰¹ç‚¹**: è®¡ç®—å¯†é›†ï¼Œå¯å¹¶è¡Œ

#### Decode é˜¶æ®µï¼ˆé€ token ç”Ÿæˆï¼‰
- **è¾“å…¥**: ä¸Šä¸€æ­¥ç”Ÿæˆçš„ token
- **è®¡ç®—**: é€ä¸ªç”Ÿæˆæ–° token
- **è¾“å‡º**: æ¯æ¬¡ç”Ÿæˆ 1 ä¸ª token
- **ç‰¹ç‚¹**: è®¿å­˜å¯†é›†ï¼Œéš¾ä»¥å¹¶è¡Œ

```
Prefill: [ä½ , å¥½, ,, ä»‹, ç»] â†’ [ä¸€]
Decode:  [ä¸€]               â†’ [ä¸‹]
Decode:  [ä¸‹]               â†’ [ä½ ]
Decode:  [ä½ ]               â†’ [è‡ª]
Decode:  [è‡ª]               â†’ [å·±]
...
```

### 2. Continuous Batchingï¼ˆè¿ç»­æ‰¹å¤„ç†ï¼‰

ä¼ ç»Ÿæ‰¹å¤„ç† vs è¿ç»­æ‰¹å¤„ç†ï¼š

#### ä¼ ç»Ÿæ‰¹å¤„ç†
```
Batch 1: [Seq1, Seq2, Seq3] â†’ ç­‰å¾…æ‰€æœ‰åºåˆ—å®Œæˆ â†’ é‡Šæ”¾
Batch 2: [Seq4, Seq5, Seq6] â†’ ç­‰å¾…æ‰€æœ‰åºåˆ—å®Œæˆ â†’ é‡Šæ”¾
```
**é—®é¢˜**: GPU ç©ºé—²æ—¶é—´é•¿ï¼ˆç­‰å¾…æœ€æ…¢çš„åºåˆ—ï¼‰

#### è¿ç»­æ‰¹å¤„ç†
```
Time 0: [Seq1, Seq2, Seq3]
Time 1: [Seq1, Seq2, Seq3, Seq4]  â† Seq4 ç«‹å³åŠ å…¥
Time 2: [Seq1, Seq2, Seq4]        â† Seq3 å®Œæˆåç«‹å³ç§»é™¤
Time 3: [Seq1, Seq2, Seq4, Seq5]  â† Seq5 ç«‹å³åŠ å…¥
```
**ä¼˜åŠ¿**: GPU åˆ©ç”¨ç‡æœ€å¤§åŒ–

### 3. Paged Attentionï¼ˆåˆ†é¡µæ³¨æ„åŠ›ï¼‰

ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜ï¼š

```mermaid
graph LR
    subgraph GPU["KV Cache ç‰©ç†å†…å­˜ (GPU)"]
        B0[å—0]
        B1[å—1]
        B2[å—2]
        B3[å—3]
        B4[å—4]
        B5[å—5]
        B6[å—6]
        B7[å—7]
    end

    S1["åºåˆ— 1<br/>å—è¡¨: [0, 3, 5]"] -.-> B0
    S1 -.-> B3
    S1 -.-> B5

    S2["åºåˆ— 2<br/>å—è¡¨: [1, 2, 4]"] -.-> B1
    S2 -.-> B2
    S2 -.-> B4

    style S1 fill:#e1f5ff
    style S2 fill:#fff4e1
    style B0 fill:#ffe1e1
    style B1 fill:#e1ffe1
    style B2 fill:#e1ffe1
    style B3 fill:#ffe1e1
    style B4 fill:#e1ffe1
    style B5 fill:#ffe1e1
```

**ä¼˜åŠ¿**:
- å†…å­˜åˆ©ç”¨ç‡é«˜ï¼ˆæ— ç¢ç‰‡åŒ–ï¼‰
- æ”¯æŒåŠ¨æ€åˆ†é…å’Œé‡Šæ”¾
- æ˜“äºå®ç° Prefix Caching

### 4. Prefix Cachingï¼ˆå‰ç¼€ç¼“å­˜ï¼‰

å…±äº«å…¬å…±å‰ç¼€çš„ KV Cacheï¼š

```
ç³»ç»Ÿæç¤ºè¯: "ä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ã€‚"ï¼ˆå ç”¨å— 0ï¼‰

è¯·æ±‚ 1: "ä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ã€‚è¯·ä»‹ç»è‡ªå·±ã€‚"
  å—è¡¨: [0, 1]  â† å— 0 æ˜¯ç³»ç»Ÿæç¤ºè¯

è¯·æ±‚ 2: "ä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ã€‚è¯·è®²ä¸ªç¬‘è¯ã€‚"
  å—è¡¨: [0, 2]  â† å¤ç”¨å— 0ï¼ˆref_count=2ï¼‰

ç»“æœ: è¯·æ±‚ 2 è·³è¿‡ç³»ç»Ÿæç¤ºè¯çš„è®¡ç®—ï¼
```

**ä¼˜åŠ¿**:
- å¤šè½®å¯¹è¯åœºæ™¯ï¼šç³»ç»Ÿæç¤ºè¯åªè®¡ç®—ä¸€æ¬¡
- æ‰¹é‡è¯·æ±‚ï¼šå…¬å…±å‰ç¼€åªè®¡ç®—ä¸€æ¬¡
- èŠ‚çœè®¡ç®—å’Œå†…å­˜

### 5. CUDA å›¾ï¼ˆCUDA Graphï¼‰

é¢„å½•åˆ¶ GPU æ“ä½œåºåˆ—ï¼š

```mermaid
graph LR
    subgraph Traditional["ä¼ ç»Ÿæ‰§è¡Œ"]
        A1[Python] --> B1["Kernel 1 å¯åŠ¨ â†’ Kernel 1 æ‰§è¡Œ"]
        B1 --> B2["Kernel 2 å¯åŠ¨ â†’ Kernel 2 æ‰§è¡Œ"]
        B2 --> B3["...<br/>æ¯ä¸ª kernel éƒ½æœ‰å¯åŠ¨å¼€é”€"]
    end

    subgraph CUDAGraph["CUDA å›¾"]
        A2[Python] --> C1["é‡æ”¾å›¾<br/>(ä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ kernel)"]
        C1 --> C2["Kernel 1, 2, 3, ... å¹¶è¡Œæ‰§è¡Œ"]
        C2 --> C3["åŠ é€Ÿæ¯”: ~30% å»¶è¿Ÿé™ä½"]
    end

    style Traditional fill:#ffe1e1
    style CUDAGraph fill:#e1ffe1
```

**é™åˆ¶**: è¾“å…¥å½¢çŠ¶å¿…é¡»å›ºå®šï¼ˆå› æ­¤ä»…åœ¨ Decode é˜¶æ®µä½¿ç”¨ï¼‰

---

## è¯¦ç»†æ–‡æ¡£

æˆ‘ä»¬æä¾›äº†å®Œæ•´çš„ä¸­æ–‡å­¦ä¹ æ–‡æ¡£ï¼Œå¸®åŠ©ä½ æ·±å…¥ç†è§£ Nano-vLLMï¼š

### ğŸ“š æ ¸å¿ƒæ–‡æ¡£

| æ–‡æ¡£ | å†…å®¹ | é€‚åˆäººç¾¤ |
|------|------|----------|
| [architecture.md](./architecture.md) | æ•´ä½“æ¶æ„ä¸æŠ€æœ¯æ ˆ | åˆå­¦è€… |
| [modules.md](./modules.md) | æ¨¡å—æ‹†åˆ†ä¸ç”¨é€”è¯¦è§£ | åˆå­¦è€… |
| [core_flows.md](./core_flows.md) | è¿è¡Œæµç¨‹ä¸ç»„ä»¶ååŒ | ä¸­çº§ |
| [code_insights.md](./code_insights.md) | æ ¸å¿ƒä»£ç è®²è§£ä¸éš¾ç‚¹è§£æ | é«˜çº§ |
| [learning_summary.md](./learning_summary.md) | æ•´ä½“å­¦ä¹ æ€»ç»“ | æ‰€æœ‰äºº |

### ğŸ¯ å­¦ä¹ è·¯å¾„

#### åˆçº§ï¼ˆç†è§£åŸç†ï¼‰
1. é˜…è¯» `architecture.md` äº†è§£æ•´ä½“æ¶æ„
2. è¿è¡Œ `example.py` ä½“éªŒæ¨ç†æµç¨‹
3. é˜…è¯» `modules.md` ç†è§£å„æ¨¡å—èŒè´£
4. æŸ¥çœ‹ `scheduler.py` å’Œ `block_manager.py` æºç 

#### ä¸­çº§ï¼ˆæ·±å…¥ç»†èŠ‚ï¼‰
1. é˜…è¯» `core_flows.md` ç†è§£æ‰§è¡Œæµç¨‹
2. è°ƒè¯•è¿è¡Œï¼Œè§‚å¯Ÿ Prefill å’Œ Decode é˜¶æ®µ
3. é˜…è¯» `model_runner.py` ç†è§£ CUDA å›¾
4. é˜…è¯» `attention.py` ç†è§£ Flash Attention

#### é«˜çº§ï¼ˆæ”¹è¿›ä¼˜åŒ–ï¼‰
1. é˜…è¯» `code_insights.md` æŒæ¡éš¾ç‚¹
2. å°è¯•æ·»åŠ æ–°æ¨¡å‹ï¼ˆå¦‚ LLaMAï¼‰
3. å®ç°æ–°ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚ Speculative Decodingï¼‰
4. é˜…è¯» `learning_summary.md` æ€»ç»“æ”¶è·

---

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæ€§èƒ½æ¯” vLLM æ›´å¥½ï¼Ÿ

**A**: ä¸»è¦åŸå› ï¼š
1. **ä»£ç ç®€æ´**: æ›´å°‘çš„æŠ½è±¡å±‚ï¼Œæ›´ç›´æ¥çš„å®ç°
2. **ä¼˜åŒ–é›†ä¸­**: ä¸“æ³¨äºæ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ï¼ˆCUDA å›¾ã€Prefix Cachingï¼‰
3. **å•ä¸€æ¨¡å‹**: ä¸éœ€è¦æ”¯æŒ 100+ æ¨¡å‹ï¼Œé¿å…é€šç”¨åŒ–å¼€é”€
4. **JIT ç¼–è¯‘**: å¤§é‡ä½¿ç”¨ `@torch.compile` ä¼˜åŒ–

**æ³¨æ„**: è¿™ä¸ªå¯¹æ¯”ä»…é’ˆå¯¹ Qwen3 æ¨¡å‹ã€‚vLLM çš„ä¼˜åŠ¿åœ¨äºé€šç”¨æ€§å’ŒåŠŸèƒ½å®Œæ•´æ€§ã€‚

### Q2: æ”¯æŒå“ªäº›æ¨¡å‹ï¼Ÿ

**A**: å½“å‰ä»…æ”¯æŒ Qwen2 å’Œ Qwen3 ç³»åˆ—æ¨¡å‹ã€‚

**æ‰©å±•å»ºè®®**:
- LLaMA: ç±»ä¼¼æ¶æ„ï¼Œæ˜“äºæ·»åŠ 
- Mistral: éœ€è¦æ”¯æŒ Sliding Window Attention
- Gemma: éœ€è¦æ”¯æŒä¸åŒçš„ Normalization

å‚è€ƒ `models/qwen3.py` å®ç°è‡ªå·±çš„æ¨¡å‹ã€‚

### Q3: å¦‚ä½•æ·»åŠ æ–°æ¨¡å‹ï¼Ÿ

**A**: æ­¥éª¤å¦‚ä¸‹ï¼š

```python
# 1. åˆ›å»ºæ¨¡å‹æ–‡ä»¶: models/your_model.py
class YourModelForCausalLM(nn.Module):
    def __init__(self, config):
        # å®ç°æ¨¡å‹æ¶æ„
        ...

    def forward(self, input_ids, positions):
        # å®ç°å‰å‘ä¼ æ’­
        ...

# 2. æ·»åŠ é…ç½®æ”¯æŒï¼ˆå¦‚æœéœ€è¦ï¼‰
# config.py: æ·»åŠ æ¨¡å‹ç‰¹å®šé…ç½®

# 3. ä¿®æ”¹åŠ è½½é€»è¾‘
# loader.py: æ”¯æŒæ–°æ¨¡å‹çš„æƒé‡åŠ è½½

# 4. æµ‹è¯•
llm = LLM("path/to/your_model", ...)
outputs = llm.generate(["Hello"], ...)
```

### Q4: å¦‚ä½•ä¼˜åŒ–æ€§èƒ½ï¼Ÿ

**A**: æ€§èƒ½è°ƒä¼˜æ£€æŸ¥æ¸…å•ï¼š

```python
# âœ… å¯ç”¨ CUDA å›¾
llm = LLM(..., enforce_eager=False)

# âœ… è°ƒæ•´ GPU å†…å­˜åˆ©ç”¨ç‡
llm = LLM(..., gpu_memory_utilization=0.9)

# âœ… ä½¿ç”¨é€‚å½“çš„å—å¤§å°
llm = LLM(..., kvcache_block_size=256)

# âœ… å¢åŠ å¹¶å‘æ•°ï¼ˆå¦‚æœæ˜¾å­˜å……è¶³ï¼‰
llm = LLM(..., max_num_seqs=512)

# âœ… å¯ç”¨å¼ é‡å¹¶è¡Œï¼ˆå¤š GPUï¼‰
llm = LLM(..., tensor_parallel_size=2)

# âœ… åˆç†è®¾ç½®æ¸©åº¦
sampling_params = SamplingParams(temperature=0.8)  # 0.1-2.0
```

### Q5: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

**A**: è§£å†³æ–¹æ¡ˆï¼š

```python
# 1. é™ä½ GPU å†…å­˜åˆ©ç”¨ç‡
llm = LLM(..., gpu_memory_utilization=0.8)  # é»˜è®¤ 0.9

# 2. å‡å°‘æœ€å¤§å¹¶å‘æ•°
llm = LLM(..., max_num_seqs=256)  # é»˜è®¤ 512

# 3. å‡å°‘æœ€å¤§æ‰¹é‡ tokens
llm = LLM(..., max_num_batched_tokens=8192)  # é»˜è®¤ 16384

# 4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
# Qwen3-0.6B è€Œé Qwen3-1.5B

# 5. ä½¿ç”¨å¼ é‡å¹¶è¡Œåˆ†æ•£æ˜¾å­˜
llm = LLM(..., tensor_parallel_size=2)
```

### Q6: å¦‚ä½•è°ƒè¯•ï¼Ÿ

**A**: è°ƒè¯•æŠ€å·§ï¼š

```python
# 1. å¯ç”¨ eager æ¨¡å¼ï¼ˆç¦ç”¨ CUDA å›¾ï¼‰
llm = LLM(..., enforce_eager=True)

# 2. æ‰“å°è°ƒåº¦ä¿¡æ¯
# ä¿®æ”¹ scheduler.py æ·»åŠ  print

# 3. æ£€æŸ¥å†…å­˜ä½¿ç”¨
import torch
print(torch.cuda.memory_summary())

# 4. æ€§èƒ½åˆ†æ
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ]
) as prof:
    llm.generate(...)

print(prof.key_averages().table(sort_by="cuda_time_total"))

# 5. å• GPU éªŒè¯
# å…ˆåœ¨å• GPU ä¸Šæµ‹è¯•ï¼Œå†æ‰©å±•åˆ°å¤š GPU
```

### Q7: ä¸ºä»€ä¹ˆä¸æ”¯æŒ Top-k/Top-p é‡‡æ ·ï¼Ÿ

**A**: ä¸ºäº†ä¿æŒä»£ç ç®€æ´ï¼Œå½“å‰ä»…å®ç°æ¸©åº¦é‡‡æ ·ã€‚

**æ‰©å±•å»ºè®®**:

```python
# layers/sampler.py

class TopKSampler(Sampler):
    @torch.compile
    def forward(self, logits, k):
        top_k_logits, top_k_indices = torch.topk(logits, k)
        probs = torch.softmax(top_k_logits, dim=-1)
        sampled_indices = torch.multinomial(probs, num_samples=1)
        return top_k_indices.gather(-1, sampled_indices)
```

---

## æ‰©å±•æ–¹å‘

### ğŸš€ åŠŸèƒ½æ‰©å±•

#### 1. æ”¯æŒæ›´å¤šæ¨¡å‹
- **LLaMA 3**: ç±»ä¼¼ Qwen3 æ¶æ„
- **Mistral 7B**: éœ€è¦ Sliding Window Attention
- **Gemma 2B**: ä¸åŒçš„ Normalization ç­–ç•¥

#### 2. æ›´å¤šé‡‡æ ·ç­–ç•¥
- **Top-k é‡‡æ ·**: ä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª tokens ä¸­é‡‡æ ·
- **Top-p é‡‡æ ·**: ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p æ—¶æˆªæ–­
- **Beam Search**: ç»´æŠ¤å¤šä¸ªå€™é€‰åºåˆ—

#### 3. é‡åŒ–æ¨ç†
- **INT8 é‡åŒ–**: å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—é‡
- **INT4 é‡åŒ–**: æ›´æè‡´çš„å‹ç¼©
- **GPTQ/AWQ**: åè®­ç»ƒé‡åŒ–æŠ€æœ¯

### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

#### 1. æµæ°´çº¿å¹¶è¡Œ
```python
# å½“å‰: ä»…å¼ é‡å¹¶è¡Œ
# ä¼˜åŒ–: å¼ é‡å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œ

# æ”¶ç›Š:
# - æ”¯æŒæ›´å¤§æ¨¡å‹ï¼ˆè·¨èŠ‚ç‚¹ï¼‰
# - æ›´é«˜ååé‡ï¼ˆé‡å è®¡ç®—å’Œé€šä¿¡ï¼‰
```

#### 2. æ¨æµ‹è§£ç ï¼ˆSpeculative Decodingï¼‰
```python
# æ€è·¯:
# 1. å°æ¨¡å‹å¿«é€Ÿç”Ÿæˆ k ä¸ª tokens
# 2. å¤§æ¨¡å‹å¹¶è¡ŒéªŒè¯
# 3. æ¥å—æ­£ç¡®çš„ tokens

# æ”¶ç›Š: 2-3x åŠ é€Ÿï¼ˆæ— ç²¾åº¦æŸå¤±ï¼‰
```

#### 3. æ··åˆç²¾åº¦
```python
# FP16/BF16 è®¡ç®— + FP32 ç´¯åŠ 
# å¹³è¡¡æ€§èƒ½å’Œç²¾åº¦
```

### ğŸ› ï¸ å·¥ç¨‹ä¼˜åŒ–

#### 1. ç›‘æ§å’Œè§‚æµ‹
- **Prometheus æŒ‡æ ‡**: ååé‡ã€å»¶è¿Ÿã€èµ„æºåˆ©ç”¨ç‡
- **OpenTelemetry è¿½è¸ª**: è¯·æ±‚é“¾è·¯è¿½è¸ª
- **Grafana Dashboard**: å®æ—¶å¯è§†åŒ–

#### 2. å®¹é”™å’Œæ¢å¤
- **Checkpoint**: å®šæœŸä¿å­˜åºåˆ—çŠ¶æ€
- **è¯·æ±‚é‡è¯•**: è‡ªåŠ¨é‡è¯•å¤±è´¥è¯·æ±‚
- **ä¼˜é›…é™çº§**: èµ„æºä¸è¶³æ—¶è‡ªåŠ¨é™çº§

#### 3. å¤šç§Ÿæˆ·æ”¯æŒ
- **èµ„æºéš”ç¦»**: ä¸åŒç§Ÿæˆ·ç‹¬ç«‹èµ„æºæ± 
- **ä¼˜å…ˆçº§é˜Ÿåˆ—**: æ”¯æŒè¯·æ±‚ä¼˜å…ˆçº§
- **é…é¢ç®¡ç†**: é™åˆ¶æ¯ä¸ªç§Ÿæˆ·çš„èµ„æºä½¿ç”¨

---

## è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿æ‰€æœ‰å½¢å¼çš„è´¡çŒ®ï¼

### ğŸ¤ å¦‚ä½•è´¡çŒ®

1. **Fork ä»“åº“**
2. **åˆ›å»ºåˆ†æ”¯**: `git checkout -b feature/your-feature`
3. **æäº¤ä»£ç **: `git commit -m "Add your feature"`
4. **æ¨é€åˆ†æ”¯**: `git push origin feature/your-feature`
5. **åˆ›å»º Pull Request**

### ğŸ“ è´¡çŒ®æ–¹å‘

#### ä»£ç è´¡çŒ®
- æ·»åŠ æ–°æ¨¡å‹æ”¯æŒ
- å®ç°æ–°ä¼˜åŒ–æŠ€æœ¯
- ä¿®å¤ Bug
- æ”¹è¿›æ€§èƒ½

#### æ–‡æ¡£è´¡çŒ®
- æ”¹è¿›ç°æœ‰æ–‡æ¡£
- æ·»åŠ ä½¿ç”¨ç¤ºä¾‹
- ç¿»è¯‘æ–‡æ¡£ï¼ˆè‹±æ–‡ â†” ä¸­æ–‡ï¼‰
- å½•åˆ¶æ•™å­¦è§†é¢‘

#### æµ‹è¯•è´¡çŒ®
- æ·»åŠ å•å…ƒæµ‹è¯•
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- å…¼å®¹æ€§æµ‹è¯•

### ğŸ¨ ä»£ç é£æ ¼

```python
# éµå¾ª PEP 8 è§„èŒƒ
# ä½¿ç”¨æœ‰æ„ä¹‰çš„å˜é‡å
# æ·»åŠ å¿…è¦çš„æ³¨é‡Šï¼ˆä½†ä¸è¦è¿‡åº¦æ³¨é‡Šï¼‰
# ä¿æŒä»£ç ç®€æ´

# ç¤ºä¾‹
def allocate_kv_cache(self, seq: Sequence):
    """ä¸ºåºåˆ—åˆ†é… KV Cache å—ã€‚

    Args:
        seq: å¾…åˆ†é…çš„åºåˆ—

    ä½¿ç”¨ Prefix Caching å°½å¯èƒ½å¤ç”¨å·²æœ‰å—ã€‚
    """
    ...
```

---

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](../LICENSE) å¼€æºã€‚

```
MIT License

Copyright (c) 2024 Xingkai Yu

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction...
```

---

## è‡´è°¢

### çµæ„Ÿæ¥æº
- **vLLM**: https://github.com/vllm-project/vllm
- **FlashAttention**: https://github.com/Dao-AILab/flash-attention
- **Triton**: https://github.com/openai/triton

### ç›¸å…³é¡¹ç›®
- **TGI**: https://github.com/huggingface/text-generation-inference
- **TensorRT-LLM**: https://github.com/NVIDIA/TensorRT-LLM
- **LMDeploy**: https://github.com/InternLM/lmdeploy

### ç‰¹åˆ«æ„Ÿè°¢
- HuggingFace æä¾›çš„ Transformers åº“
- Qwen å›¢é˜Ÿå¼€æºçš„ä¼˜ç§€æ¨¡å‹
- æ‰€æœ‰è´¡çŒ®è€…å’Œ Star æ”¯æŒè€…

---

## è”ç³»æ–¹å¼

- **GitHub Issues**: https://github.com/GeeeekExplorer/nano-vllm/issues
- **GitHub Discussions**: https://github.com/GeeeekExplorer/nano-vllm/discussions
- **Email**: [é¡¹ç›®ä½œè€…é‚®ç®±]

---

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&type=Date)](https://star-history.com/#GeeeekExplorer/nano-vllm&Date)

---

<div align="center">

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸€ä¸ª â­ Starï¼**

**Let's build amazing things together! ğŸš€**

</div>

---

*æœ€åæ›´æ–°: 2025-12-25*
*ä½œè€…: Claude (Code Reading Mentor)*
