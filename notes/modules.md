# Nano-vLLM æ¨¡å—æ‹†åˆ†ä¸ç”¨é€”è¯¦è§£

## ç›®å½•
1. [æ¨¡å—æ€»è§ˆ](#æ¨¡å—æ€»è§ˆ)
2. [Engine æ¨¡å— - æ¨ç†å¼•æ“æ ¸å¿ƒ](#engine-æ¨¡å—---æ¨ç†å¼•æ“æ ¸å¿ƒ)
3. [Layers æ¨¡å— - ç¥ç»ç½‘ç»œå±‚](#layers-æ¨¡å—---ç¥ç»ç½‘ç»œå±‚)
4. [Models æ¨¡å— - æ¨¡å‹æ¶æ„](#models-æ¨¡å—---æ¨¡å‹æ¶æ„)
5. [Utils æ¨¡å— - å·¥å…·æ¨¡å—](#utils-æ¨¡å—---å·¥å…·æ¨¡å—)
6. [é…ç½®æ¨¡å—](#é…ç½®æ¨¡å—)
7. [æ¨¡å—ä¾èµ–å…³ç³»å›¾](#æ¨¡å—ä¾èµ–å…³ç³»å›¾)

---

## æ¨¡å—æ€»è§ˆ

Nano-vLLM é‡‡ç”¨**åˆ†å±‚æ¨¡å—åŒ–è®¾è®¡**ï¼Œå…±åˆ†ä¸º 5 å¤§æ ¸å¿ƒæ¨¡å—ï¼š

| æ¨¡å— | ç›®å½• | æ–‡ä»¶æ•° | æ ¸å¿ƒèŒè´£ | é‡è¦æ€§ |
|------|------|--------|----------|--------|
| **Engine** | `nanovllm/engine/` | 5 | æ¨ç†å¼•æ“æ§åˆ¶ã€è°ƒåº¦ã€èµ„æºç®¡ç† | â˜…â˜…â˜…â˜…â˜… |
| **Layers** | `nanovllm/layers/` | 7 | å¯å¤ç”¨çš„ç¥ç»ç½‘ç»œç®—å­ | â˜…â˜…â˜…â˜…â˜… |
| **Models** | `nanovllm/models/` | 1 | å…·ä½“æ¨¡å‹æ¶æ„å®ç° | â˜…â˜…â˜…â˜…â˜… |
| **Utils** | `nanovllm/utils/` | 2 | è¾…åŠ©å·¥å…·ï¼ˆä¸Šä¸‹æ–‡ã€æƒé‡åŠ è½½ï¼‰ | â˜…â˜…â˜…â˜… |
| **Config** | `nanovllm/` | 3 | é…ç½®ç®¡ç†å’Œç”¨æˆ·æ¥å£ | â˜…â˜…â˜… |

---

## Engine æ¨¡å— - æ¨ç†å¼•æ“æ ¸å¿ƒ

**ç›®å½•**: `nanovllm/engine/`
**ä½œç”¨**: æ¨ç†å¼•æ“çš„æ ¸å¿ƒé€»è¾‘ï¼Œè´Ÿè´£è¯·æ±‚è°ƒåº¦ã€èµ„æºç®¡ç†ã€æ¨¡å‹æ‰§è¡Œ

### ğŸ“ æ–‡ä»¶æ¸…å•

#### 1. `llm_engine.py` (93 è¡Œ) â­â­â­â­â­
**ä½œç”¨**: æ¨ç†å¼•æ“ä¸»æ§åˆ¶å™¨ï¼Œæ˜¯æ•´ä¸ªæ¨ç†æµç¨‹çš„ä¸­å¤®åè°ƒè€…

**æ ¸å¿ƒèŒè´£**:
- âœ… å¤šè¿›ç¨‹å¼ é‡å¹¶è¡Œç®¡ç†ï¼ˆé€šè¿‡ `multiprocessing.spawn`ï¼‰
- âœ… åè°ƒ `Scheduler` å’Œ `ModelRunner`
- âœ… åˆ†è¯ï¼ˆTokenizationï¼‰å’Œè§£ç ï¼ˆDetokenizationï¼‰
- âœ… æ‰¹é‡æ¨ç†æ¥å£ï¼ˆ`generate()`ï¼‰
- âœ… è¿›åº¦æ¡æ˜¾ç¤ºå’Œååé‡ç»Ÿè®¡

**å…³é”®ç±»ä¸æ–¹æ³•**:

```python
class LLMEngine:
    def __init__(self, model, **kwargs):
        # åˆå§‹åŒ–é…ç½®
        # å¯åŠ¨å¤šè¿›ç¨‹å¼ é‡å¹¶è¡Œ worker
        # åˆ›å»º Scheduler å’Œ ModelRunner
        # åŠ è½½ Tokenizer

    def add_request(self, prompt, sampling_params):
        # å°†ç”¨æˆ·è¯·æ±‚æ·»åŠ åˆ°è°ƒåº¦é˜Ÿåˆ—

    def step(self):
        # æ‰§è¡Œä¸€æ­¥æ¨ç†ï¼ˆPrefill æˆ– Decodeï¼‰
        # 1. è°ƒåº¦å™¨è°ƒåº¦åºåˆ—
        # 2. ModelRunner æ‰§è¡Œå‰å‘ä¼ æ’­
        # 3. åå¤„ç†ï¼ˆæ›´æ–°åºåˆ—çŠ¶æ€ï¼‰
        # è¿”å›ï¼šå®Œæˆçš„åºåˆ— + token æ•°é‡

    def generate(self, prompts, sampling_params):
        # æ‰¹é‡ç”Ÿæˆæ¥å£
        # å¾ªç¯è°ƒç”¨ step() ç›´åˆ°æ‰€æœ‰åºåˆ—å®Œæˆ
```

**å¤šè¿›ç¨‹æ¶æ„**:

```mermaid
graph TD
    subgraph "ä¸»è¿›ç¨‹ rank=0"
        A[LLMEngine]
        B[Scheduler]
        C[ModelRunner rank=0]
    end

    subgraph "å­è¿›ç¨‹ rank=1,2,...,N-1"
        D1[ModelRunner rank=1]
        D2[ModelRunner rank=2]
        D3[ModelRunner rank=N-1]
    end

    A --> B
    A --> C

    C -.å…±äº«å†…å­˜.-> D1
    C -.å…±äº«å†…å­˜.-> D2
    C -.å…±äº«å†…å­˜.-> D3

    C -.EventåŒæ­¥.-> D1
    C -.EventåŒæ­¥.-> D2
    C -.EventåŒæ­¥.-> D3

    style A fill:#fff4e1
    style B fill:#ffe1f5
    style C fill:#e1ffe1
    style D1 fill:#e1ffe1
    style D2 fill:#e1ffe1
    style D3 fill:#e1ffe1
```

**é€šä¿¡æ–¹å¼**:
- å…±äº«å†…å­˜ï¼ˆè¾“å…¥/è¾“å‡º tensorsï¼‰
- multiprocessing.Eventï¼ˆåŒæ­¥ä¿¡å·ï¼‰

**æ€§èƒ½ç›‘æ§**:
- Prefill ååé‡ï¼ˆtokens/sï¼‰
- Decode ååé‡ï¼ˆtokens/sï¼‰
- å®æ—¶è¿›åº¦æ¡ï¼ˆtqdmï¼‰

---

#### 2. `model_runner.py` (251 è¡Œ) â­â­â­â­â­ **[æœ€å¤§æ–‡ä»¶]**
**ä½œç”¨**: æ¨¡å‹æ‰§è¡Œå™¨ï¼Œè´Ÿè´£å®é™…çš„æ¨¡å‹æ¨ç†è®¡ç®—

**æ ¸å¿ƒèŒè´£**:
- âœ… CUDA å›¾æ•è·å’Œé‡æ”¾ï¼ˆæ€§èƒ½ä¼˜åŒ–å…³é”®ï¼‰
- âœ… KV Cache å†…å­˜åˆ†é…å’Œç®¡ç†
- âœ… è¾“å…¥æ•°æ®å‡†å¤‡ï¼ˆinput_ids, positions, slot_mappingï¼‰
- âœ… å¼ é‡å¹¶è¡Œé€šä¿¡ï¼ˆNCCL All-Reduceï¼‰
- âœ… Prefill/Decode é˜¶æ®µåˆ‡æ¢
- âœ… å…±äº«å†…å­˜å¤šè¿›ç¨‹é€šä¿¡

**å…³é”®ç±»ä¸æ–¹æ³•**:

```python
class ModelRunner:
    def __init__(self, config, rank, events):
        # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒï¼ˆNCCLï¼‰
        # åŠ è½½æ¨¡å‹å’Œæƒé‡
        # åˆ†é… KV Cache æ˜¾å­˜
        # é¢„çƒ­æ¨¡å‹å’Œæ•è· CUDA å›¾

    def call(self, method, *args):
        # ä¸»è¿›ç¨‹ä¸å­è¿›ç¨‹é€šä¿¡çš„ç»Ÿä¸€æ¥å£
        # æ–¹æ³•ï¼šrun, exit

    def run(self, seqs, is_prefill):
        # æ‰§è¡Œæ¨ç†
        # Prefill: ç›´æ¥å‰å‘ä¼ æ’­
        # Decode: ä¼˜å…ˆä½¿ç”¨ CUDA å›¾é‡æ”¾

    def capture_cudagraph(self, num_seqs):
        # æ•è·ç‰¹å®š batch size çš„ CUDA å›¾
        # é¢„åˆ†é…å›ºå®šå¤§å°çš„è¾“å…¥è¾“å‡º tensors

    def allocate_kv_cache(self):
        # åŠ¨æ€è®¡ç®—å’Œåˆ†é… KV Cache æ˜¾å­˜
        # è€ƒè™‘æ¨¡å‹å‚æ•°ã€æ¿€æ´»å€¼ã€GPU å†…å­˜åˆ©ç”¨ç‡
```

**CUDA å›¾ä¼˜åŒ–**:
```python
# æ•è·é˜¶æ®µï¼ˆwarmup æ—¶ï¼‰
with torch.cuda.graph(cuda_graph):
    output = model.forward(fixed_inputs)

# é‡æ”¾é˜¶æ®µï¼ˆå®é™…æ¨ç†æ—¶ï¼‰
input_buffer.copy_(actual_input)  # æ‹·è´è¾“å…¥åˆ°å›ºå®šç¼“å†²åŒº
cuda_graph.replay()               # é‡æ”¾ CUDA å›¾
result = output_buffer.clone()    # ä»å›ºå®šç¼“å†²åŒºè¯»å–è¾“å‡º
```

**å†…å­˜ç®¡ç†ç­–ç•¥**:
- å‚æ•°å†…å­˜ = `model_params_bytes`
- æ¿€æ´»å†…å­˜ = `max_num_batched_tokens * hidden_size * dtype_size * N_layers`
- KV Cache å†…å­˜ = `gpu_memory * utilization - params - activation`

---

#### 3. `scheduler.py` (71 è¡Œ) â­â­â­â­
**ä½œç”¨**: è¯·æ±‚è°ƒåº¦å™¨ï¼Œå†³å®šå“ªäº›åºåˆ—åœ¨ä»€ä¹ˆæ—¶å€™æ‰§è¡Œ

**æ ¸å¿ƒèŒè´£**:
- âœ… ç®¡ç†ç­‰å¾…é˜Ÿåˆ—ï¼ˆ`waiting`ï¼‰å’Œè¿è¡Œé˜Ÿåˆ—ï¼ˆ`running`ï¼‰
- âœ… åŠ¨æ€è°ƒåº¦ Prefill å’Œ Decode é˜¶æ®µ
- âœ… èµ„æºæŠ¢å ï¼ˆPreemptionï¼‰æœºåˆ¶
- âœ… æ‰¹å¤„ç†ä¼˜åŒ–ï¼ˆæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡ï¼‰

**è°ƒåº¦ç­–ç•¥**:

```python
class Scheduler:
    def schedule(self):
        # é˜¶æ®µ 1: ä¼˜å…ˆè°ƒåº¦ Prefill è¯·æ±‚
        if waiting é˜Ÿåˆ—éç©º:
            while èµ„æºå……è¶³:
                - æ£€æŸ¥åºåˆ—é•¿åº¦æ˜¯å¦è¶…è¿‡ max_num_batched_tokens
                - æ£€æŸ¥ KV Cache æ˜¯å¦è¶³å¤Ÿåˆ†é…
                - åˆ†é… KV Cache å—
                - å°†åºåˆ—ç§»è‡³ running é˜Ÿåˆ—
            return prefill_seqs, is_prefill=True

        # é˜¶æ®µ 2: è°ƒåº¦ Decode è¯·æ±‚
        for seq in running:
            if KV Cache ä¸è¶³ä»¥è¿½åŠ æ–° token:
                æŠ¢å ä½ä¼˜å…ˆçº§åºåˆ—ï¼ˆä»é˜Ÿå°¾å¼€å§‹ï¼‰
            else:
                é¢„åˆ†é…ä¸‹ä¸€ä¸ª token çš„ KV Cache slot
        return decode_seqs, is_prefill=False

    def preempt(self, seq):
        # æŠ¢å åºåˆ—ï¼šé‡Šæ”¾ KV Cacheï¼Œç§»å› waiting é˜Ÿåˆ—

    def postprocess(self, seqs, token_ids):
        # æ›´æ–°åºåˆ—çŠ¶æ€
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åœæ­¢æ¡ä»¶ï¼ˆEOS æˆ– max_tokensï¼‰
        # é‡Šæ”¾å®Œæˆåºåˆ—çš„ KV Cache
```

**è°ƒåº¦ä¼˜å…ˆçº§**:
1. **Prefill > Decode**: æ–°è¯·æ±‚ä¼˜å…ˆå¤„ç†ï¼ˆå‡å°‘ç”¨æˆ·ç­‰å¾…æ—¶é—´ï¼‰
2. **FIFO**: åŒç±»è¯·æ±‚æŒ‰å…ˆæ¥å…ˆæœåŠ¡
3. **æŠ¢å ç­–ç•¥**: ä» running é˜Ÿåˆ—å°¾éƒ¨å¼€å§‹æŠ¢å ï¼ˆåè¿›å…ˆå‡ºï¼‰

**èµ„æºé™åˆ¶**:
- `max_num_seqs`: æœ€å¤§å¹¶å‘åºåˆ—æ•°ï¼ˆé»˜è®¤ 512ï¼‰
- `max_num_batched_tokens`: å•æ‰¹æ¬¡æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ 16384ï¼‰

---

#### 4. `sequence.py` (83 è¡Œ) â­â­â­â­
**ä½œç”¨**: åºåˆ—æ•°æ®ç»“æ„ï¼Œè¡¨ç¤ºå•ä¸ªæ¨ç†è¯·æ±‚çš„å®Œæ•´çŠ¶æ€

**æ ¸å¿ƒèŒè´£**:
- âœ… ç®¡ç† token IDsï¼ˆprompt + completionï¼‰
- âœ… ç»´æŠ¤åºåˆ—çŠ¶æ€ï¼ˆWAITING/RUNNING/FINISHEDï¼‰
- âœ… å­˜å‚¨å—è¡¨ï¼ˆBlock Tableï¼‰
- âœ… æ”¯æŒåºåˆ—åŒ–/ååºåˆ—åŒ–ï¼ˆå¤šè¿›ç¨‹é€šä¿¡ï¼‰

**æ•°æ®ç»“æ„**:

```python
class Sequence:
    seq_id: int                      # å”¯ä¸€æ ‡è¯†ç¬¦
    prompt_token_ids: list[int]      # è¾“å…¥ prompt
    completion_token_ids: list[int]  # ç”Ÿæˆçš„ tokens
    status: SequenceStatus           # WAITING/RUNNING/FINISHED
    block_table: list[int]           # KV Cache å—ç´¢å¼•è¡¨
    num_cached_tokens: int           # Prefix Cache å‘½ä¸­çš„ token æ•°

    # é‡‡æ ·å‚æ•°
    temperature: float
    max_tokens: int
    ignore_eos: bool

    def append_token(self, token_id):
        # è¿½åŠ æ–°ç”Ÿæˆçš„ token

    def block(self, idx) -> list[int]:
        # è·å–ç¬¬ idx ä¸ªå—çš„ token IDs

    @property
    def num_blocks(self):
        # è®¡ç®—éœ€è¦çš„ KV Cache å—æ•°é‡
```

**åºåˆ—çŠ¶æ€æœº**:

```mermaid
stateDiagram-v2
    [*] --> WAITING
    WAITING --> RUNNING: schedule
    RUNNING --> FINISHED: finish
    RUNNING --> WAITING: preempt
    FINISHED --> [*]
```

**å—è¡¨ï¼ˆBlock Tableï¼‰ç¤ºä¾‹**:
```
åºåˆ—é•¿åº¦ = 48 tokens, å—å¤§å° = 16
block_table = [5, 12, 7]

å— 5: tokens[0:16]
å— 12: tokens[16:32]
å— 7: tokens[32:48]
```

---

#### 5. `block_manager.py` (112 è¡Œ) â­â­â­â­â­
**ä½œç”¨**: KV Cache å—ç®¡ç†å™¨ï¼Œå®ç° Prefix Caching

**æ ¸å¿ƒèŒè´£**:
- âœ… å—çº§ KV Cache åˆ†é…å’Œå›æ”¶
- âœ… Prefix Cachingï¼ˆåŸºäºå“ˆå¸Œçš„å—å…±äº«ï¼‰
- âœ… å¼•ç”¨è®¡æ•°ç®¡ç†ï¼ˆCopy-on-Writeï¼‰
- âœ… å“ˆå¸Œç¢°æ’æ£€æµ‹

**æ ¸å¿ƒæ•°æ®ç»“æ„**:

```python
class Block:
    block_id: int           # å— ID
    ref_count: int          # å¼•ç”¨è®¡æ•°
    hash: int               # token åºåˆ—å“ˆå¸Œå€¼
    token_ids: list[int]    # ç¼“å­˜çš„ token IDsï¼ˆç”¨äºç¢°æ’æ£€æµ‹ï¼‰

class BlockManager:
    blocks: list[Block]                # æ‰€æœ‰å—
    hash_to_block_id: dict[int, int]   # å“ˆå¸Œ â†’ å— ID æ˜ å°„
    free_block_ids: deque[int]         # ç©ºé—²å—é˜Ÿåˆ—
    used_block_ids: set[int]           # å·²ä½¿ç”¨å—é›†åˆ
```

**Prefix Caching ç®—æ³•**:

```python
def allocate(self, seq):
    h = -1
    cache_miss = False
    for i in range(seq.num_blocks):
        token_ids = seq.block(i)

        # è®¡ç®—å¢é‡å“ˆå¸Œ
        if len(token_ids) == block_size:
            h = compute_hash(token_ids, prefix=h)

        # æŸ¥æ‰¾å“ˆå¸Œè¡¨
        block_id = hash_to_block_id.get(h, -1)

        # éªŒè¯å“ˆå¸Œï¼ˆé˜²æ­¢ç¢°æ’ï¼‰
        if block_id != -1 and blocks[block_id].token_ids == token_ids:
            # ç¼“å­˜å‘½ä¸­ï¼
            if block_id in used_block_ids:
                # å—å·²è¢«ä½¿ç”¨ï¼Œå¢åŠ å¼•ç”¨è®¡æ•°
                blocks[block_id].ref_count += 1
            else:
                # å—æœªä½¿ç”¨ï¼Œé‡æ–°åˆ†é…
                allocate_block(block_id)
            seq.num_cached_tokens += block_size
        else:
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œåˆ†é…æ–°å—
            cache_miss = True
            block_id = free_block_ids[0]
            allocate_block(block_id)

        # æ›´æ–°å“ˆå¸Œè¡¨
        blocks[block_id].update(h, token_ids)
        hash_to_block_id[h] = block_id
        seq.block_table.append(block_id)
```

**å¼•ç”¨è®¡æ•°ï¼ˆCopy-on-Writeï¼‰**:
- `ref_count = 1`: ç‹¬å å—
- `ref_count > 1`: å…±äº«å—ï¼ˆåªè¯»ï¼‰
- `ref_count = 0`: ç©ºé—²å—

**ä¼˜åŠ¿**:
- å¤šè½®å¯¹è¯ä¸­å¤ç”¨ç³»ç»Ÿæç¤ºè¯çš„ KV Cache
- æ‰¹é‡è¯·æ±‚ä¸­å…±äº«å…¬å…±å‰ç¼€
- å‡å°‘å†—ä½™è®¡ç®—å’Œå†…å­˜å ç”¨

**ç¤ºä¾‹**:
```
è¯·æ±‚ 1: "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚è¯·ä»‹ç»è‡ªå·±ã€‚"
è¯·æ±‚ 2: "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚è¯·è®²ä¸ªç¬‘è¯ã€‚"

å…¬å…±å‰ç¼€ "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚" çš„ KV Cache å—è¢«å…±äº«
ref_count = 2
```

---

## Layers æ¨¡å— - ç¥ç»ç½‘ç»œå±‚

**ç›®å½•**: `nanovllm/layers/`
**ä½œç”¨**: æä¾›å¯å¤ç”¨çš„ç¥ç»ç½‘ç»œç®—å­ï¼Œæ”¯æŒå¼ é‡å¹¶è¡Œå’Œé«˜æ€§èƒ½ä¼˜åŒ–

### ğŸ“ æ–‡ä»¶æ¸…å•

#### 1. `attention.py` (75 è¡Œ) â­â­â­â­â­
**ä½œç”¨**: Flash Attention é›†æˆï¼Œå¤„ç†æ³¨æ„åŠ›è®¡ç®—

**æ ¸å¿ƒæŠ€æœ¯**:
- **Prefill é˜¶æ®µ**: `flash_attn_varlen_func`ï¼ˆå˜é•¿åºåˆ—æ‰¹å¤„ç†ï¼‰
- **Decode é˜¶æ®µ**: `flash_attn_with_kvcache`ï¼ˆç›´æ¥è¯»å†™ KV Cacheï¼‰
- **Triton å†…æ ¸**: `store_kvcache_kernel`ï¼ˆé«˜æ€§èƒ½ KV Cache å­˜å‚¨ï¼‰

**å…³é”®ä»£ç **:

```python
@triton.jit
def store_kvcache_kernel(...):
    # ä½¿ç”¨ Triton ç¼–å†™çš„ GPU å†…æ ¸
    # å°† key/value å­˜å‚¨åˆ° KV Cache
    idx = tl.program_id(0)
    slot = tl.load(slot_mapping_ptr + idx)
    key = tl.load(key_ptr + ...)
    value = tl.load(value_ptr + ...)
    tl.store(k_cache_ptr + cache_offsets, key)
    tl.store(v_cache_ptr + cache_offsets, value)

class Attention(nn.Module):
    def forward(self, q, k, v):
        # 1. å­˜å‚¨ KV åˆ° Cache
        store_kvcache(k, v, k_cache, v_cache, slot_mapping)

        if context.is_prefill:
            # 2a. Prefill: å˜é•¿åºåˆ—æ‰¹å¤„ç†
            o = flash_attn_varlen_func(
                q, k, v,
                cu_seqlens_q=...,    # ç´¯ç§¯åºåˆ—é•¿åº¦
                max_seqlen_q=...,    # æœ€å¤§åºåˆ—é•¿åº¦
                causal=True,          # å› æœæ©ç 
                block_table=...       # å—è¡¨ï¼ˆç”¨äº Prefix Cacheï¼‰
            )
        else:
            # 2b. Decode: ä» KV Cache è¯»å–
            o = flash_attn_with_kvcache(
                q.unsqueeze(1),       # [batch, 1, num_heads, head_dim]
                k_cache, v_cache,
                cache_seqlens=...,    # æ¯ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡é•¿åº¦
                block_table=...,      # å—è¡¨
                causal=True
            )
        return o
```

**ä¼˜åŠ¿**:
- O(N) å†…å­˜å¤æ‚åº¦ï¼ˆvs æ ‡å‡† Attention çš„ O(NÂ²)ï¼‰
- IO ä¼˜åŒ–ï¼ˆå‡å°‘ HBM â†” SRAM æ•°æ®ä¼ è¾“ï¼‰
- ç›´æ¥æ”¯æŒ Paged Attentionï¼ˆé€šè¿‡ `block_table`ï¼‰

---

#### 2. `linear.py` (153 è¡Œ) â­â­â­â­â­
**ä½œç”¨**: å¼ é‡å¹¶è¡Œçº¿æ€§å±‚

**æ ¸å¿ƒç±»**:

##### `ColumnParallelLinear` (åˆ—åˆ‡åˆ†)
```python
# æƒé‡åˆ‡åˆ†æ–¹å¼ï¼š
# å®Œæ•´æƒé‡: [output_size, input_size]
# GPU 0: [output_size/N, input_size]
# GPU 1: [output_size/N, input_size]
# ...
# GPU N-1: [output_size/N, input_size]

# å‰å‘ä¼ æ’­ï¼š
input: [batch, seq_len, input_size]
output: [batch, seq_len, output_size/N]  # æ¯ä¸ª GPU äº§ç”Ÿéƒ¨åˆ†è¾“å‡º
# æ— éœ€é€šä¿¡ï¼
```

##### `RowParallelLinear` (è¡Œåˆ‡åˆ† + All-Reduce)
```python
# æƒé‡åˆ‡åˆ†æ–¹å¼ï¼š
# å®Œæ•´æƒé‡: [output_size, input_size]
# GPU 0: [output_size, input_size/N]
# GPU 1: [output_size, input_size/N]
# ...

# å‰å‘ä¼ æ’­ï¼š
input: [batch, seq_len, input_size]
local_output = F.linear(input, weight)  # [batch, seq_len, output_size]
output = all_reduce(local_output)       # è·¨ GPU æ±‚å’Œ
```

##### `QKVParallelLinear` (ä¸“ç”¨äºæ³¨æ„åŠ›æŠ•å½±)
```python
# åŒæ—¶è®¡ç®— Q, K, V ä¸‰ä¸ªæŠ•å½±
# æƒé‡å¸ƒå±€: [num_heads * 3 * head_dim, hidden_size]
# åˆ‡åˆ†ç»´åº¦: num_headsï¼ˆä¿æŒ head_dim å®Œæ•´ï¼‰
```

##### `MergedColumnParallelLinear` (åˆå¹¶é—¨æ§æŠ•å½±)
```python
# ç”¨äº MLP çš„ gate_proj å’Œ up_proj èåˆ
# æƒé‡å¸ƒå±€: [(gate_size + up_size), hidden_size]
# å‡å°‘ kernel å¯åŠ¨æ¬¡æ•°
```

**æƒé‡åŠ è½½æœºåˆ¶**:
```python
def weight_loader(self, param, loaded_weight):
    # è‡ªåŠ¨åˆ‡ç‰‡æƒé‡åˆ°å¯¹åº”çš„ GPU
    shard_size = param.size(tp_dim)
    start_idx = tp_rank * shard_size
    loaded_weight = loaded_weight.narrow(tp_dim, start_idx, shard_size)
    param.data.copy_(loaded_weight)
```

---

#### 3. `layernorm.py` (50 è¡Œ) â­â­â­
**ä½œç”¨**: RMSNorm å½’ä¸€åŒ–å±‚ï¼ˆå¸¦æ®‹å·®èåˆï¼‰

**æ ¸å¿ƒå®ç°**:

```python
@torch.compile
def rmsnorm_with_residual(hidden, residual, weight, eps):
    # èåˆ RMSNorm å’Œæ®‹å·®è¿æ¥
    # é¿å…é¢å¤–çš„ kernel å¯åŠ¨å’Œå†…å­˜è¯»å†™
    orig_dtype = hidden.dtype
    hidden = hidden + residual            # æ®‹å·®è¿æ¥
    residual = hidden.to(torch.float32)   # ä¿å­˜æ®‹å·®

    # RMSNorm
    variance = hidden.pow(2).mean(-1, keepdim=True)
    hidden = hidden * torch.rsqrt(variance + eps)
    return weight * hidden.to(orig_dtype), residual

class RMSNorm(nn.Module):
    def forward(self, x, residual=None):
        if residual is not None:
            return rmsnorm_with_residual(x, residual, self.weight, self.eps)
        else:
            return rmsnorm(x, self.weight, self.eps)
```

**ä¼˜åŒ–**:
- `@torch.compile`: å³æ—¶ç¼–è¯‘ä¸ºä¼˜åŒ–çš„ CUDA ä»£ç 
- æ®‹å·®èåˆ: å‡å°‘ 30% çš„å†…å­˜è®¿é—®
- æ··åˆç²¾åº¦: è®¡ç®—ä½¿ç”¨ float32ï¼Œå­˜å‚¨ä½¿ç”¨ float16/bfloat16

---

#### 4. `rotary_embedding.py` (61 è¡Œ) â­â­â­
**ä½œç”¨**: æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰

**æ ¸å¿ƒå®ç°**:

```python
@torch.compile
def apply_rotary_emb(q, k, cos, sin, cos_k, sin_k):
    # åº”ç”¨æ—‹è½¬çŸ©é˜µ
    # q' = q * cos + rotate_half(q) * sin
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos_k) + (rotate_half(k) * sin_k)
    return q_embed, k_embed

def rotate_half(x):
    # å°†ç‰¹å¾åˆ†ä¸ºä¸¤åŠå¹¶æ—‹è½¬
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)
```

**ä¼˜åŠ¿**:
- ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆæ”¯æŒä»»æ„é•¿åº¦æ¨ç†ï¼‰
- æ— éœ€é¢å¤–å‚æ•°ï¼ˆä»…å­˜å‚¨é¢„è®¡ç®—çš„ cos/sin è¡¨ï¼‰
- `@torch.compile` ä¼˜åŒ–

---

#### 5. `sampler.py` (15 è¡Œ) â­â­
**ä½œç”¨**: Token é‡‡æ ·å™¨

**æ ¸å¿ƒå®ç°**:

```python
@torch.compile
def sample(logits, temperatures):
    # æ¸©åº¦ç¼©æ”¾ + å¤šé¡¹å¼é‡‡æ ·
    logits = logits / temperatures[:, None]
    probs = torch.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1).squeeze(1)

class Sampler(nn.Module):
    def forward(self, logits, temperatures):
        # logits: [batch_size, vocab_size]
        # temperatures: [batch_size]
        # æ”¯æŒæ¯ä¸ªåºåˆ—ç‹¬ç«‹çš„æ¸©åº¦å‚æ•°
        return sample(logits, temperatures)
```

**ç‰¹ç‚¹**:
- ä»…æ”¯æŒæ¸©åº¦é‡‡æ ·ï¼ˆä¸æ”¯æŒ Top-k/Top-pï¼‰
- æ¯ä¸ªåºåˆ—ç‹¬ç«‹æ¸©åº¦å‚æ•°
- `@torch.compile` åŠ é€Ÿ

---

#### 6. `activation.py` (14 è¡Œ) â­â­
**ä½œç”¨**: æ¿€æ´»å‡½æ•°ï¼ˆSiluAndMulï¼‰

```python
class SiluAndMul(nn.Module):
    def forward(self, x):
        # é—¨æ§æœºåˆ¶ï¼šgate * activation(input)
        # x: [batch, seq_len, 2 * intermediate_size]
        gate, x = x.chunk(2, dim=-1)
        return F.silu(gate) * x
```

ç”¨äº Qwen3 çš„ MLP é—¨æ§æœºåˆ¶ã€‚

---

#### 7. `embed_head.py` (66 è¡Œ) â­â­â­
**ä½œç”¨**: è¯åµŒå…¥å’Œè¯­è¨€æ¨¡å‹å¤´

**æ ¸å¿ƒç±»**:

##### `VocabParallelEmbedding` (è¯åµŒå…¥)
```python
# è¯è¡¨åˆ‡åˆ†ï¼š
# GPU 0: vocab[0 : vocab_size/N]
# GPU 1: vocab[vocab_size/N : 2*vocab_size/N]
# ...

def forward(self, input_ids):
    # ä»…æŸ¥æ‰¾æœ¬ GPU å¯¹åº”çš„è¯è¡¨èŒƒå›´
    mask = (input_ids >= start_idx) & (input_ids < end_idx)
    local_ids = input_ids - start_idx
    embeddings = F.embedding(local_ids * mask)
    return all_reduce(embeddings)  # è·¨ GPU æ±‚å’Œ
```

##### `ParallelLMHead` (è¾“å‡ºæŠ•å½±)
```python
# ç±»ä¼¼ VocabParallelEmbedding çš„åå‘æ“ä½œ
# æ¯ä¸ª GPU è®¡ç®—éƒ¨åˆ†è¯è¡¨çš„ logits
def forward(self, hidden_states):
    logits = F.linear(hidden_states, self.weight)  # éƒ¨åˆ† logits
    return logits  # ä¸éœ€è¦ all_reduceï¼ˆé‡‡æ ·æ—¶ä»…éœ€æœ¬åœ° logitsï¼‰
```

---

## Models æ¨¡å— - æ¨¡å‹æ¶æ„

**ç›®å½•**: `nanovllm/models/`
**ä½œç”¨**: å…·ä½“æ¨¡å‹æ¶æ„çš„å®ç°

### ğŸ“ `qwen3.py` (215 è¡Œ) â­â­â­â­â­

**æ”¯æŒæ¨¡å‹**:
- Qwen3-0.6B
- Qwen3-1.5B
- Qwen2ï¼ˆå…¼å®¹ï¼‰

**æ¶æ„ç»„ä»¶**:

#### `Qwen3Attention` (å¤šå¤´æ³¨æ„åŠ›)
```python
class Qwen3Attention(nn.Module):
    def __init__(self):
        # GQA (Grouped Query Attention)
        self.num_heads = 16           # Query å¤´æ•°
        self.num_kv_heads = 2         # Key/Value å¤´æ•°ï¼ˆæ›´å°‘ï¼‰

        # æŠ•å½±å±‚
        self.qkv_proj = QKVParallelLinear(...)
        self.o_proj = RowParallelLinear(...)

        # ä½ç½®ç¼–ç 
        self.rotary_emb = RotaryEmbedding(...)

        # æ³¨æ„åŠ›
        self.attn = Attention(...)

    def forward(self, hidden_states, positions):
        # 1. QKV æŠ•å½±
        qkv = self.qkv_proj(hidden_states)
        q, k, v = qkv.split([...])

        # 2. åº”ç”¨ RoPE
        q, k = self.rotary_emb(q, k, positions)

        # 3. æ³¨æ„åŠ›è®¡ç®—
        attn_output = self.attn(q, k, v)

        # 4. è¾“å‡ºæŠ•å½±
        output = self.o_proj(attn_output)
        return output
```

#### `Qwen3MLP` (é—¨æ§ FFN)
```python
class Qwen3MLP(nn.Module):
    def __init__(self):
        # èåˆ gate_proj å’Œ up_proj
        self.gate_up_proj = MergedColumnParallelLinear(
            hidden_size,
            2 * intermediate_size  # 2x æ˜¯å› ä¸ºèåˆäº†ä¸¤ä¸ªæŠ•å½±
        )
        self.down_proj = RowParallelLinear(...)
        self.act_fn = SiluAndMul()

    def forward(self, x):
        # gate_up: [batch, seq, 2 * intermediate_size]
        gate_up = self.gate_up_proj(x)

        # æ¿€æ´»ï¼šgate * silu(input)
        x = self.act_fn(gate_up)

        # ä¸‹æŠ•å½±
        x = self.down_proj(x)
        return x
```

#### `Qwen3DecoderLayer` (Transformer å±‚)
```python
class Qwen3DecoderLayer(nn.Module):
    def forward(self, hidden_states, positions, residual):
        # 1. Pre-Norm Attention
        hidden_states, residual = self.input_layernorm(hidden_states, residual)
        hidden_states = self.self_attn(hidden_states, positions)

        # 2. Pre-Norm MLP
        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
        hidden_states = self.mlp(hidden_states)

        return hidden_states, residual
```

#### `Qwen3ForCausalLM` (å®Œæ•´æ¨¡å‹)
```python
class Qwen3ForCausalLM(nn.Module):
    def forward(self, input_ids, positions):
        # 1. è¯åµŒå…¥
        hidden_states = self.embed_tokens(input_ids)
        residual = None

        # 2. Transformer å±‚
        for layer in self.layers:
            hidden_states, residual = layer(hidden_states, positions, residual)

        # 3. æœ€ç»ˆå½’ä¸€åŒ–
        hidden_states, _ = self.norm(hidden_states, residual)

        # 4. è¾“å‡ºæŠ•å½±ï¼ˆä»…å¯¹æœ€åä¸€ä¸ª tokenï¼‰
        hidden_states = hidden_states[context.last_token_indices]
        logits = self.lm_head(hidden_states)

        return logits
```

**æ¶æ„ç‰¹ç‚¹**:
- **Pre-Norm**: å½’ä¸€åŒ–åœ¨å­å±‚ä¹‹å‰ï¼ˆæ›´ç¨³å®šè®­ç»ƒï¼‰
- **GQA**: Grouped Query Attentionï¼ˆå‡å°‘ KV Cache å†…å­˜ï¼‰
- **RoPE**: æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆæ”¯æŒé•¿åºåˆ—å¤–æ¨ï¼‰
- **é—¨æ§ MLP**: SiluAndMul æ¿€æ´»ï¼ˆæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼‰

---

## Utils æ¨¡å— - å·¥å…·æ¨¡å—

**ç›®å½•**: `nanovllm/utils/`
**ä½œç”¨**: æä¾›è¾…åŠ©åŠŸèƒ½

### ğŸ“ æ–‡ä»¶æ¸…å•

#### 1. `context.py` (27 è¡Œ) â­â­â­
**ä½œç”¨**: å…¨å±€ä¸Šä¸‹æ–‡ç®¡ç†ï¼Œåœ¨å„å±‚é—´ä¼ é€’å…ƒæ•°æ®

**æ ¸å¿ƒå®ç°**:

```python
_context = None

class Context:
    is_prefill: bool

    # Prefill é˜¶æ®µ
    cu_seqlens_q: torch.Tensor      # ç´¯ç§¯åºåˆ—é•¿åº¦ï¼ˆQueryï¼‰
    cu_seqlens_k: torch.Tensor      # ç´¯ç§¯åºåˆ—é•¿åº¦ï¼ˆKeyï¼‰
    max_seqlen_q: int               # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆQueryï¼‰
    max_seqlen_k: int               # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆKeyï¼‰

    # Decode é˜¶æ®µ
    context_lens: torch.Tensor      # æ¯ä¸ªåºåˆ—çš„ä¸Šä¸‹æ–‡é•¿åº¦

    # å…±äº«
    block_tables: torch.Tensor      # å—è¡¨
    slot_mapping: torch.Tensor      # Slot æ˜ å°„
    last_token_indices: torch.Tensor # æœ€åä¸€ä¸ª token çš„ç´¢å¼•

def set_context(ctx: Context):
    global _context
    _context = ctx

def get_context() -> Context:
    return _context
```

**ä½¿ç”¨åœºæ™¯**:
- Attention å±‚æ ¹æ® `is_prefill` é€‰æ‹©ä¸åŒçš„è®¡ç®—è·¯å¾„
- æ¨¡å‹å±‚è¯»å– `last_token_indices` æ¥ä»…å¤„ç†æœ€åä¸€ä¸ª token
- å­˜å‚¨ `block_tables` ä¾› Flash Attention ä½¿ç”¨

**ä¼˜åŠ¿**:
- é¿å…åœ¨å‡½æ•°ç­¾åä¸­ä¼ é€’å¤§é‡å‚æ•°
- å…¨å±€å¯è®¿é—®ï¼ˆç±»ä¼¼çº¿ç¨‹å±€éƒ¨å­˜å‚¨ï¼‰

---

#### 2. `loader.py` (28 è¡Œ) â­â­â­
**ä½œç”¨**: æ¨¡å‹æƒé‡åŠ è½½å™¨

**æ ¸å¿ƒå®ç°**:

```python
def load_weights(model, model_path):
    # 1. åŠ è½½ SafeTensors æƒé‡
    weights = load_file(f"{model_path}/model.safetensors")

    # 2. å¤„ç†æƒé‡æ˜ å°„ï¼ˆæ‰“åŒ…æ¨¡å—ï¼‰
    # ä¾‹å¦‚ï¼šgate_up_proj éœ€è¦æ‹†åˆ†ä¸º gate_proj å’Œ up_proj
    for name, param in model.named_parameters():
        if hasattr(param, 'weight_loader'):
            # è‡ªå®šä¹‰åŠ è½½é€»è¾‘ï¼ˆç”¨äºå¼ é‡å¹¶è¡Œåˆ‡ç‰‡ï¼‰
            param.weight_loader(param, weights[name])
        else:
            # ç›´æ¥å¤åˆ¶
            param.data.copy_(weights[name])
```

**æƒé‡æ˜ å°„ç¤ºä¾‹**:
```python
# SafeTensors ä¸­çš„æƒé‡å
weights["layers.0.mlp.gate_up_proj.weight"]  # [2 * intermediate_size, hidden_size]

# æ¨¡å‹ä¸­çš„å‚æ•°å
model.layers[0].mlp.gate_proj.weight   # [intermediate_size, hidden_size]
model.layers[0].mlp.up_proj.weight     # [intermediate_size, hidden_size]

# åŠ è½½æ—¶è‡ªåŠ¨æ‹†åˆ†
gate_weight = weights[...].narrow(0, 0, intermediate_size)
up_weight = weights[...].narrow(0, intermediate_size, intermediate_size)
```

---

## é…ç½®æ¨¡å—

**ç›®å½•**: `nanovllm/`
**ä½œç”¨**: é…ç½®ç®¡ç†å’Œç”¨æˆ·æ¥å£

### ğŸ“ æ–‡ä»¶æ¸…å•

#### 1. `config.py` (26 è¡Œ) â­â­â­
**ä½œç”¨**: å…¨å±€é…ç½®ç±»

```python
@dataclass
class Config:
    # æ¨¡å‹è·¯å¾„
    model: str

    # æ‰¹å¤„ç†é…ç½®
    max_num_batched_tokens: int = 16384   # å•æ‰¹æ¬¡æœ€å¤§ token æ•°
    max_num_seqs: int = 512               # æœ€å¤§å¹¶å‘åºåˆ—æ•°
    max_model_len: int = 4096             # æœ€å¤§åºåˆ—é•¿åº¦

    # èµ„æºé…ç½®
    gpu_memory_utilization: float = 0.9   # GPU å†…å­˜åˆ©ç”¨ç‡
    tensor_parallel_size: int = 1         # å¼ é‡å¹¶è¡Œåº¦

    # ä¼˜åŒ–é…ç½®
    enforce_eager: bool = False           # ç¦ç”¨ CUDA å›¾ï¼ˆè°ƒè¯•ç”¨ï¼‰

    # KV Cache é…ç½®
    kvcache_block_size: int = 256         # å—å¤§å°ï¼ˆå¿…é¡»æ˜¯ 256 çš„å€æ•°ï¼‰
    num_kvcache_blocks: int = -1          # å—æ•°é‡ï¼ˆ-1 è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼‰

    # è¿è¡Œæ—¶é…ç½®
    hf_config: AutoConfig = None          # HuggingFace é…ç½®
    eos: int = -1                         # EOS token ID
```

**é…ç½®éªŒè¯**:
```python
def __post_init__(self):
    assert os.path.isdir(self.model)
    assert self.kvcache_block_size % 256 == 0
    assert 1 <= self.tensor_parallel_size <= 8
    self.hf_config = AutoConfig.from_pretrained(self.model)
    self.max_model_len = min(self.max_model_len, self.hf_config.max_position_embeddings)
```

---

#### 2. `sampling_params.py` (11 è¡Œ) â­â­
**ä½œç”¨**: é‡‡æ ·å‚æ•°å®šä¹‰

```python
@dataclass
class SamplingParams:
    temperature: float = 1.0      # æ¸©åº¦ï¼ˆ> 0, è¶Šä½è¶Šç¡®å®šæ€§ï¼‰
    max_tokens: int = 16          # æœ€å¤§ç”Ÿæˆé•¿åº¦
    ignore_eos: bool = False      # æ˜¯å¦å¿½ç•¥ EOS token
```

---

#### 3. `llm.py` (5 è¡Œ) â­
**ä½œç”¨**: ç”¨æˆ·æ¥å£åŒ…è£…

```python
class LLM(LLMEngine):
    # ç®€å•åŒ…è£… LLMEngineï¼Œæä¾›æ›´ç®€æ´çš„æ¥å£
    pass
```

---

## æ¨¡å—ä¾èµ–å…³ç³»å›¾

### å±‚æ¬¡ä¾èµ–

```mermaid
graph TD
    subgraph "ç”¨æˆ·æ¥å£å±‚"
        A[llm.py LLM]
    end

    subgraph "å¼•æ“å±‚"
        B[engine/llm_engine.py]
        C[engine/scheduler.py]
        D[engine/block_manager.py]
        E[engine/sequence.py]
        F[engine/model_runner.py]
    end

    subgraph "æ¨¡å‹å±‚"
        G[models/qwen3.py]
        H[layers/embed_head.py]
        I[layers/layernorm.py]
        J[Qwen3DecoderLayer]
    end

    subgraph "Qwen3DecoderLayer å†…éƒ¨"
        K[layers/attention.py Qwen3Attention]
        L[layers/linear.py QKV/Row Parallel]
        M[layers/rotary_embedding.py]
        N[layers/attention.py Attention]
        O[Qwen3MLP]
        P[layers/linear.py Merged/Row Parallel]
        Q[layers/activation.py]
    end

    subgraph "å·¥å…·å±‚"
        R[utils/context.py]
        S[utils/loader.py]
    end

    subgraph "é…ç½®å±‚"
        T[config.py]
        U[sampling_params.py]
    end

    A --> B
    B --> C
    B --> F
    C --> D
    D --> E

    F --> G
    G --> H
    G --> I
    G --> J

    J --> K
    J --> O
    K --> L
    K --> M
    K --> N
    O --> P
    O --> Q

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#ffe1f5
    style D fill:#ffe1f5
    style F fill:#e1ffe1
    style G fill:#f5e1ff
```

### æ¨¡å—é—´é€šä¿¡

```mermaid
sequenceDiagram
    actor User as ç”¨æˆ·
    participant LLM as LLM.generate
    participant Engine as LLMEngine
    participant Tokenizer
    participant Scheduler
    participant BlockMgr as BlockManager
    participant Runner as ModelRunner
    participant Context
    participant Model as Qwen3ForCausalLM
    participant Layers

    User->>LLM: generate(prompts)
    LLM->>Engine: å¼€å§‹æ¨ç†

    Engine->>Tokenizer: encode(prompts)
    Tokenizer-->>Engine: token_ids

    Engine->>Scheduler: schedule()
    Scheduler->>BlockMgr: allocate(seq)
    BlockMgr-->>Scheduler: block_table

    Engine->>Runner: run(seqs, is_prefill)
    Runner->>Context: set_context(metadata)
    Runner->>Model: forward(input_ids, positions)
    Model->>Layers: Attention, MLP, etc.
    Layers-->>Model: output
    Model-->>Runner: logits

    Engine->>Scheduler: postprocess(seqs, token_ids)
    Scheduler->>BlockMgr: deallocate(seq)

    Engine-->>LLM: è¿”å›ç»“æœ
    LLM-->>User: generated_text
```

---

## æ€»ç»“

Nano-vLLM çš„æ¨¡å—è®¾è®¡éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š

1. **èŒè´£æ¸…æ™°**: æ¯ä¸ªæ¨¡å—ä¸“æ³¨äºå•ä¸€èŒè´£
2. **é«˜å†…èšä½è€¦åˆ**: æ¨¡å—é—´é€šè¿‡æ¸…æ™°çš„æ¥å£é€šä¿¡
3. **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°æ¨¡å‹ã€æ–°ç®—å­ã€æ–°è°ƒåº¦ç­–ç•¥
4. **æ€§èƒ½ä¼˜å…ˆ**: å¤§é‡ä½¿ç”¨ä¼˜åŒ–æŠ€æœ¯ï¼ˆCUDA å›¾ã€Tritonã€torch.compileï¼‰
5. **æ˜“äºç†è§£**: ä»£ç ç®€æ´ï¼Œç»“æ„æ¸…æ™°

**æ ¸å¿ƒæ¨¡å—é‡è¦æ€§æ’åº**:
1. **Engine (å¼•æ“)**: æ¨ç†æµç¨‹çš„ä¸­æ¢
2. **Layers (ç®—å­)**: æ€§èƒ½ä¼˜åŒ–çš„å…³é”®
3. **Models (æ¨¡å‹)**: æ¶æ„å®ç°çš„æ ¸å¿ƒ
4. **Utils (å·¥å…·)**: è¾…åŠ©åŠŸèƒ½çš„æ”¯æ’‘
5. **Config (é…ç½®)**: çµæ´»æ€§çš„ä¿éšœ

æ¨èå­¦ä¹ è·¯å¾„ï¼š**Config â†’ Layers â†’ Models â†’ Engine â†’ Utils**
